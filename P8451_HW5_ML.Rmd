---
title: "P8451_HW5_ML"
author: "Ruixi Li"
date: "2024-02-14"
output: html_document
---
# Introduction

Goal: You want to predict current alcohol consumption but it is expensive and time-consuming to administer all of the behavioral testing that produces the personality scores. You will conduct a reproducible analysis to build and test classification models using regularized logistic regression and traditional logistic regression. You will produce a shareable report that includes code, results and answers to questions using R Markdown.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r libarary, include=FALSE}
library(tidyverse) 
library(forcats)
library(caret)
library(glmnet)
library(klaR)
library(gtsummary)
```

# Data preparation 

```{r data_pre, message=FALSE, warning=FALSE}
set.seed(123)
alcohol_use = read_csv("alcohol_use.csv") 
skimr::skim(alcohol_use)
alcohol_use = alcohol_use[,-1]
alcohol_use$alc_consumption = as.factor(alcohol_use$alc_consumption)

#tidyverse way to create data partition
train.indices <- alcohol_use %>%
  pull(alc_consumption) %>%
  createDataPartition(p = 0.7, list = FALSE)

train.data <- alcohol_use %>%
  slice(train.indices)

test.data <- alcohol_use %>%
  slice(-train.indices)

control = trainControl(method = "repeatedcv", 
                      number = 10,
                      repeats = 5,
                      selectionFunction = "best")
```

# Model building

```{r elasticnet_training}
en.model<- train(
                  alc_consumption ~., 
                  data = train.data, 
                  method = "glmnet",
                  trControl =  control, 
                  preProc=c("center", "scale"),
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(6, 0, length = 100)))
                )
#Print the values of alpha and lambda that gave best prediction
en.model$bestTune


# Model coefficients
coef(en.model$finalModel, en.model$bestTune$lambda)

confusionMatrix(en.model)


```


```{r logistic_training}

# Define logistic regression model
log.model <- train(
                  alc_consumption ~ .,
                  data = train.data,
                  method = "glm",
                  trControl = control)

confusionMatrix(log.model)

#Print all of the options examined
log.model$results

# Model coefficients
log.model$finalModel |> tbl_regression()
summary(log.model)$coef


```


```{r lasso_training}
set.seed(123)

#Create grid to search lambda
lambda<-10^seq(-3,3, length=100)

#Note replacing tuneLength with tuneGrid
la.model<-train(
                alc_consumption ~., 
                data=train.data, 
                method="glmnet", 
                trControl=control, 
                preProc=c("center", "scale"), 
                tuneGrid=expand.grid(alpha=1, lambda=lambda)
              )

confusionMatrix(la.model)

#Print the values of lambda that gave best prediction
la.model$bestTune

# Model coefficients
coef(la.model$finalModel, la.model$bestTune$lambda) 


```

# Model comparison

```{r compare_model}
models = list(model1 = en.model, model2 = log.model, model3 = la.model)
results <- resamples(models)
summary(results)
```
Since the accuracy and kappa for model3(logistic regression with lasso penalty) are the highest among the 3 models, I would choose model 3(logistic regression with lasso penalty) as my final model.

# Model evaluation

```{r}
la.pred <- la.model %>% 
              predict(test.data)

# Model prediction performance
confusion_matrix = confusionMatrix(la.pred,test.data$alc_consumption)
confusion_matrix

#visualization
cm = confusion_matrix$table
cm_melted <- as.data.frame(as.table(cm))
ggplot(data = cm_melted, aes(x = Reference, y = Prediction, fill = Freq)) +
    geom_tile() +
    geom_text(aes(label = sprintf("%d", Freq)), vjust = 1) +
    scale_fill_gradient(low = "white", high = "blue") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(fill = "Count", title = "Confusion Matrix", x = "Actual label", y = "Predicted label")

```

* Since the outcome is binary, I used confusionMatrix to evaluate this model. Accuracy is about 85.5%, PPV is about 78.6%, sensitivity is 100%, specifiicty is about 68.9%. These metrics suggest that the model is highly accurate and specific but could improve in terms of specificity to reduce the number of false negatives. 

# Research questions

* a) this model can directly address the research question that which features most strongly predict recent alcohol consumption (currentuse/noncurrentuse), because lasso regression can select features.  b) this model help address research question exploring whether interactions between different personality traits increase the predictive power of the model.